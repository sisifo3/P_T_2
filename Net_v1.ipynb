{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Net_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JxTLbfuVbdK7gQYEmYmRVXZH2Q0XqN_A",
      "authorship_tag": "ABX9TyMzO9RNfqPesOncpYxndgV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sisifo3/P_T_2/blob/main/Net_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6aXRlrhJTh"
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS42bze-hckY"
      },
      "source": [
        "\n",
        "class NoisyImages():\n",
        "    def __init__(self,transform=None):\n",
        "        #self.dataset = pd.read_csv(path_to_trainset, sep=' ')\n",
        "        self.dataset = pd.read_csv('/content/drive/MyDrive/train_circle_v4/train_set_v2.csv', sep=' ')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.load(self.dataset.iloc[idx, 0].split(',')[0])\n",
        "        target = [self.dataset.iloc[idx, 0].split(',')[i] for i in range(1, 4)]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = np.expand_dims(np.asarray(image), axis=0)\n",
        "            image = torch.from_numpy(np.array(image, dtype=np.float32))\n",
        "            target = torch.from_numpy(np.array(np.asarray(target), dtype=np.float32))\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZETRMDDNP6WZ"
      },
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda:0'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "device = get_device()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIwd5iOPI07I"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # input images 1 * 200 * 200\n",
        "        self.L1 = nn.Sequential(nn.Conv2d(1, 32, 5), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2))  # 32 * 98 * 98\n",
        "        self.L2 = nn.Sequential(nn.Conv2d(32, 64, 3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2))  # 64 * 48 * 48\n",
        "        self.L3 = nn.Sequential(nn.Conv2d(64, 128, 3), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2))  # 128 * 23 * 23\n",
        "        self.L4 = nn.Sequential(nn.Conv2d(128, 128, 3), nn.BatchNorm2d(128), nn.ReLU())  # 128 * 21 * 21\n",
        "        self.L5 = nn.Sequential(nn.Conv2d(128, 4, 1), nn.BatchNorm2d(4), nn.ReLU())  # 4 * 21 * 21\n",
        "        self.FC = nn.Sequential(nn.Linear(4 * 21* 21, 256), nn.ReLU(), nn.Linear(256, 16), nn.ReLU())\n",
        "        self.Last = nn.Linear(16, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.L5(self.L4(self.L3(self.L2(self.L1(x)))))\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(-1, C * H * W)\n",
        "        x = self.FC(x)\n",
        "        x = self.Last(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJdliCC0ObGK"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate\"\"\"\n",
        "    lr = 0.001\n",
        "    if 20 < epoch <= 30:\n",
        "        lr = 0.0001\n",
        "    elif 30 < epoch :\n",
        "        lr = 0.00001\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    print(\"learning rate -> {}\\n\".format(lr))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRxPLv7lhm3J",
        "outputId": "03a1ee05-80af-4287-9b13-6b8e1fcec709"
      },
      "source": [
        "def main():\n",
        "\n",
        "    model = Net()\n",
        "    #model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion1 = nn.MSELoss()\n",
        "    criterion2 = nn.L1Loss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr =0.001)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "\n",
        "    trainset = NoisyImages(transforms.Compose([normalize,]))\n",
        "\n",
        "    print(trainset)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size= 4, shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "    #output = open(args.out_file, \"w\")\n",
        "    for epoch in range(2):\n",
        "        print('Epoch {}'.format(epoch))\n",
        "\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        print(enumerate(train_loader))\n",
        "\n",
        "        for i, (images, target) in enumerate(train_loader):\n",
        "\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "\n",
        "            loss = criterion1(output, target/200) + 0.1 * criterion2(output, target/200)\n",
        "\n",
        "            # compute gradient and do optimizer step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            print('Loss: {:.4f}'.format(loss))\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.NoisyImages object at 0x7fa3a421abd0>\n",
            "Epoch 0\n",
            "learning rate -> 0.001\n",
            "\n",
            "<enumerate object at 0x7fa3a4228230>\n",
            "Loss: 0.1372\n",
            "Loss: 0.3246\n",
            "Loss: 0.4597\n",
            "Loss: 0.6933\n",
            "Loss: 0.8136\n",
            "Loss: 0.9329\n",
            "Loss: 1.0724\n",
            "Loss: 1.1903\n",
            "Loss: 1.3443\n",
            "Loss: 1.4483\n",
            "Loss: 1.5135\n",
            "Loss: 1.6687\n",
            "Loss: 1.7404\n",
            "Loss: 1.7903\n",
            "Loss: 1.8980\n",
            "Loss: 1.9909\n",
            "Loss: 2.0811\n",
            "Loss: 2.1474\n",
            "Loss: 2.2337\n",
            "Loss: 2.3230\n",
            "Loss: 2.4355\n",
            "Loss: 2.4948\n",
            "Loss: 2.5886\n",
            "Loss: 2.7038\n",
            "Loss: 2.8224\n",
            "Loss: 2.9209\n",
            "Loss: 3.0667\n",
            "Loss: 3.1545\n",
            "Loss: 3.2371\n",
            "Loss: 3.3251\n",
            "Loss: 3.4527\n",
            "Loss: 3.6130\n",
            "Loss: 3.8348\n",
            "Loss: 3.8651\n",
            "Loss: 3.9646\n",
            "Loss: 4.1556\n",
            "Loss: 4.2199\n",
            "Loss: 4.2912\n",
            "Loss: 4.3701\n",
            "Loss: 4.4504\n",
            "Loss: 4.5991\n",
            "Loss: 4.7071\n",
            "Loss: 4.7640\n",
            "Loss: 4.8392\n",
            "Loss: 4.9098\n",
            "Loss: 5.0009\n",
            "Loss: 5.1159\n",
            "Loss: 5.2961\n",
            "Loss: 5.3793\n",
            "Loss: 5.4651\n",
            "Loss: 5.5612\n",
            "Loss: 5.6348\n",
            "Loss: 5.7184\n",
            "Loss: 5.8179\n",
            "Loss: 5.9953\n",
            "Loss: 6.0867\n",
            "Loss: 6.1515\n",
            "Loss: 6.2384\n",
            "Loss: 6.2969\n",
            "Loss: 6.3772\n",
            "Loss: 6.4797\n",
            "Loss: 6.5494\n",
            "Loss: 6.6508\n",
            "Loss: 6.8125\n",
            "Loss: 6.9199\n",
            "Loss: 6.9827\n",
            "Loss: 7.0879\n",
            "Loss: 7.1801\n",
            "Loss: 7.2720\n",
            "Loss: 7.3993\n",
            "Loss: 7.5013\n",
            "Loss: 7.6355\n",
            "Loss: 7.7165\n",
            "Loss: 7.8231\n",
            "Loss: 7.9322\n",
            "Loss: 8.0082\n",
            "Loss: 8.0804\n",
            "Loss: 8.1510\n",
            "Loss: 8.2546\n",
            "Loss: 8.3586\n",
            "Loss: 8.5110\n",
            "Loss: 8.5930\n",
            "Loss: 8.6969\n",
            "Loss: 8.7629\n",
            "Loss: 8.8399\n",
            "Loss: 8.9142\n",
            "Loss: 9.0413\n",
            "Loss: 9.1467\n",
            "Loss: 9.2185\n",
            "Loss: 9.3572\n",
            "Loss: 9.4157\n",
            "Loss: 9.4735\n",
            "Loss: 9.5397\n",
            "Loss: 9.6247\n",
            "Loss: 9.6927\n",
            "Loss: 9.7593\n",
            "Loss: 9.8182\n",
            "Loss: 9.9278\n",
            "Loss: 10.0848\n",
            "Loss: 10.1416\n",
            "Loss: 10.2438\n",
            "Loss: 10.3020\n",
            "Loss: 10.3811\n",
            "Loss: 10.4993\n",
            "Loss: 10.6151\n",
            "Loss: 10.7320\n",
            "Loss: 10.8024\n",
            "Loss: 10.8900\n",
            "Loss: 10.9707\n",
            "Loss: 11.1288\n",
            "Loss: 11.1871\n",
            "Loss: 11.2804\n",
            "Loss: 11.3578\n",
            "Loss: 11.4346\n",
            "Loss: 11.5136\n",
            "Loss: 11.5728\n",
            "Loss: 11.6843\n",
            "Loss: 11.7695\n",
            "Loss: 11.8548\n",
            "Loss: 11.9541\n",
            "Loss: 12.0760\n",
            "Loss: 12.1544\n",
            "Loss: 12.2199\n",
            "Loss: 12.3095\n",
            "Loss: 12.4278\n",
            "Loss: 12.4593\n",
            "Loss: 12.5647\n",
            "Loss: 12.6298\n",
            "Loss: 12.7298\n",
            "Loss: 12.8065\n",
            "Loss: 12.8835\n",
            "Loss: 12.9255\n",
            "Loss: 13.0115\n",
            "Loss: 13.1263\n",
            "Loss: 13.2300\n",
            "Loss: 13.3218\n",
            "Loss: 13.3801\n",
            "Loss: 13.4550\n",
            "Loss: 13.5360\n",
            "Loss: 13.6298\n",
            "Loss: 13.7027\n",
            "Loss: 13.7485\n",
            "Loss: 13.8592\n",
            "Loss: 13.9594\n",
            "Loss: 14.0538\n",
            "Loss: 14.1565\n",
            "Loss: 14.2048\n",
            "Loss: 14.2675\n",
            "Loss: 14.3413\n",
            "Loss: 14.4436\n",
            "Loss: 14.5280\n",
            "Loss: 14.6165\n",
            "Loss: 14.6641\n",
            "Loss: 14.7621\n"
          ]
        }
      ]
    }
  ]
}